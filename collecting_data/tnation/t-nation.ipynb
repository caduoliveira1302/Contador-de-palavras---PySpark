{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from time import sleep\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tNation:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.t-nation.com/\"\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.maximize_window()\n",
    "        #there are 20 articles per page\n",
    "        self.articles_path = [f\"\"\"/html/body/div[4]/div[3]/div/div/main/div/div/article[{i}]/div/div[2]/header/h2/a\"\"\" for i in range(1,21)]\n",
    "        # self.paragraphs_path = [f\"\"\"/html/body/div[4]/div[3]/div/div/div/main/article/div/div/div[1]/p[{i}]\"\"\" for i in range(1,21)]\n",
    "        self.final_collection_list = []\n",
    "\n",
    "    def open_url(self):\n",
    "        self.driver.get(self.url)\n",
    "\n",
    "    def search_product(self):\n",
    "        articles_menu = self.driver.find_element(By.XPATH, \"\"\"//*[@id=\"primary-menu\"]/li[2]/a\"\"\")\n",
    "        articles_menu.click()\n",
    "        sleep(2)\n",
    "\n",
    "        search_input = self.driver.find_element(By.XPATH, \"\"\"/html/body/div[4]/div[3]/div/div[2]/main/div/div/div[1]/div/form/input\"\"\")\n",
    "        search_input.send_keys(\"whey protein\")\n",
    "        search_button = self.driver.find_element(By.XPATH, \"\"\"/html/body/div[4]/div[3]/div/div[2]/main/div/div/div[1]/div/form/button\"\"\")\n",
    "        search_button.click()\n",
    "        sleep(3)\n",
    "\n",
    "        for artic in self.articles_path:\n",
    "            xpath_article = self.driver.find_element(By.XPATH, f\"{artic}\")\n",
    "            xpath_article.click()\n",
    "            sleep(2)\n",
    "                \n",
    "            entire_page = self.driver.find_element(By.XPATH, \"/html/body/div[4]/div[3]/div/div/div/main/article/div/div/div[1]\")\n",
    "            self.final_collection_list.append(entire_page.text)\n",
    "            self.driver.back()\n",
    "\n",
    "    def save_articles(self):\n",
    "        df = pd.DataFrame({\"articles_text\" : self.final_collection_list})\n",
    "        df.to_csv(f\"tnation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnation = tNation()\n",
    "tnation.open_url()\n",
    "tnation.search_product()\n",
    "tnation.save_articles()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRÓXIMOS PASSOS\n",
    "- Coletar das páginas 2,3,4,5...\n",
    "- Coletar em português (consigo traduzir a página com webscraping?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
