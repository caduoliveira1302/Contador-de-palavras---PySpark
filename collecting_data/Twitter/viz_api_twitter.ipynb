{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cedua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode, col, trim, rtrim, ltrim\n",
    "from pyspark.sql.functions import regexp_replace, regexp_extract\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserScreenName</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text</th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Image link</th>\n",
       "      <th>Tweet URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kamii</td>\n",
       "      <td>@kami_whey</td>\n",
       "      <td>2022-06-15T22:28:20.000Z</td>\n",
       "      <td>kamii\\n@kami_whey\\n¬∑\\nJun 15, 2022</td>\n",
       "      <td>i will not give context, i just find this imag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>['https://pbs.twimg.com/media/FVU7ZmGWAAIqepU?...</td>\n",
       "      <td>https://twitter.com/kami_whey/status/153720035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tia feri.</td>\n",
       "      <td>@feristhii</td>\n",
       "      <td>2022-06-15T21:29:34.000Z</td>\n",
       "      <td>tia feri.\\n@feristhii\\n¬∑\\nJun 15, 2022</td>\n",
       "      <td>Apenas um treino usando uma gostosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>869</td>\n",
       "      <td>['https://pbs.twimg.com/media/FVUuIaeWUAA13vp?...</td>\n",
       "      <td>https://twitter.com/feristhii/status/153718557...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monty Riches</td>\n",
       "      <td>@MontyRiches</td>\n",
       "      <td>2022-06-15T22:34:27.000Z</td>\n",
       "      <td>Monty Riches\\n@MontyRiches\\n¬∑\\nJun 15, 2022</td>\n",
       "      <td>MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...</td>\n",
       "      <td>üö® ü¶ß üö® üí™ ü¶ç</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>['https://pbs.twimg.com/media/FVU8-5cXsAEsOqc?...</td>\n",
       "      <td>https://twitter.com/MontyRiches/status/1537201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Walk Like an Egyptian Bot</td>\n",
       "      <td>@egyptian_bot</td>\n",
       "      <td>2022-06-15T22:10:49.000Z</td>\n",
       "      <td>Walk Like an Egyptian Bot\\n@egyptian_bot\\n¬∑\\nJ...</td>\n",
       "      <td>Foreign types with the hookah pipes say ay oh ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/egyptian_bot/status/153719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thomas the gain</td>\n",
       "      <td>@thomasphall</td>\n",
       "      <td>2022-06-15T20:49:42.000Z</td>\n",
       "      <td>thomas the gain\\n@thomasphall\\n¬∑\\nJun 15, 2022</td>\n",
       "      <td>I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...</td>\n",
       "      <td>üöÇ üòÇ</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/thomasphall/status/1537175...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              UserScreenName       UserName                 Timestamp  \\\n",
       "0                      kamii     @kami_whey  2022-06-15T22:28:20.000Z   \n",
       "1                  tia feri.     @feristhii  2022-06-15T21:29:34.000Z   \n",
       "2               Monty Riches   @MontyRiches  2022-06-15T22:34:27.000Z   \n",
       "3  Walk Like an Egyptian Bot  @egyptian_bot  2022-06-15T22:10:49.000Z   \n",
       "4            thomas the gain   @thomasphall  2022-06-15T20:49:42.000Z   \n",
       "\n",
       "                                                Text  \\\n",
       "0                 kamii\\n@kami_whey\\n¬∑\\nJun 15, 2022   \n",
       "1             tia feri.\\n@feristhii\\n¬∑\\nJun 15, 2022   \n",
       "2        Monty Riches\\n@MontyRiches\\n¬∑\\nJun 15, 2022   \n",
       "3  Walk Like an Egyptian Bot\\n@egyptian_bot\\n¬∑\\nJ...   \n",
       "4     thomas the gain\\n@thomasphall\\n¬∑\\nJun 15, 2022   \n",
       "\n",
       "                                       Embedded_text     Emojis Comments  \\\n",
       "0  i will not give context, i just find this imag...        NaN        1   \n",
       "1                Apenas um treino usando uma gostosa        NaN        3   \n",
       "2  MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...  üö® ü¶ß üö® üí™ ü¶ç        4   \n",
       "3  Foreign types with the hookah pipes say ay oh ...        NaN      NaN   \n",
       "4  I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...        üöÇ üòÇ        2   \n",
       "\n",
       "  Likes Retweets                                         Image link  \\\n",
       "0   NaN        3  ['https://pbs.twimg.com/media/FVU7ZmGWAAIqepU?...   \n",
       "1    58      869  ['https://pbs.twimg.com/media/FVUuIaeWUAA13vp?...   \n",
       "2     3       19  ['https://pbs.twimg.com/media/FVU8-5cXsAEsOqc?...   \n",
       "3   NaN      NaN                                                 []   \n",
       "4   NaN      NaN                                                 []   \n",
       "\n",
       "                                           Tweet URL  \n",
       "0  https://twitter.com/kami_whey/status/153720035...  \n",
       "1  https://twitter.com/feristhii/status/153718557...  \n",
       "2  https://twitter.com/MontyRiches/status/1537201...  \n",
       "3  https://twitter.com/egyptian_bot/status/153719...  \n",
       "4  https://twitter.com/thomasphall/status/1537175...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter = pd.read_csv('C:\\\\\\\\Users\\\\\\\\cedua\\\\\\\\CDIA - PUCSP\\\\\\\\PROJETO PySpark - SAVINO\\\\\\\\Contador-de-palavras---PySpark\\\\\\\\collecting_data\\\\\\\\twitter_whey_V3.csv')\n",
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i will not give context, i just find this imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apenas um treino usando uma gostosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foreign types with the hookah pipes say ay oh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>treino assim pra quem n√£o sabe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8414</th>\n",
       "      <td>FELLOW GAYMERS RISE UP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8415</th>\n",
       "      <td>Shit is about to get incredibly annoying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8416</th>\n",
       "      <td>O TREINO INOVADOR DO CORINTHIANS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8417</th>\n",
       "      <td>Calories in Egg whites Vs Whey Protein.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8418 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Embedded_text\n",
       "0     i will not give context, i just find this imag...\n",
       "1                   Apenas um treino usando uma gostosa\n",
       "2     MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...\n",
       "3     Foreign types with the hookah pipes say ay oh ...\n",
       "4     I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...\n",
       "...                                                 ...\n",
       "8413                     treino assim pra quem n√£o sabe\n",
       "8414                             FELLOW GAYMERS RISE UP\n",
       "8415           Shit is about to get incredibly annoying\n",
       "8416                   O TREINO INOVADOR DO CORINTHIANS\n",
       "8417            Calories in Egg whites Vs Whey Protein.\n",
       "\n",
       "[8418 rows x 1 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ONLY THE TEXT\n",
    "df_twitter_text = df_twitter[['Embedded_text']]\n",
    "df_twitter_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedua\\AppData\\Local\\Temp\\ipykernel_10500\\2608503998.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_twitter_text['lang'] = df_twitter_text['Embedded_text'].apply(detect_language)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i will not give context, i just find this imag...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apenas um treino usando uma gostosa</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foreign types with the hookah pipes say ay oh ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Embedded_text lang\n",
       "0  i will not give context, i just find this imag...   en\n",
       "1                Apenas um treino usando uma gostosa   pt\n",
       "2  MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...   en\n",
       "3  Foreign types with the hookah pipes say ay oh ...   en\n",
       "4  I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...   en"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREATING A NEW FEATURE TO INDENTIFY THE LANGUAGE OF THE TEXT\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_twitter_text['lang'] = df_twitter_text['Embedded_text'].apply(detect_language)\n",
    "df_twitter_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE -- > (8418, 2)\n",
      "HOW MANY LANGUAGES -- > 32\n",
      "{'sq', 'fr', 'ja', 'tr', 'nl', 'th', 'de', 'ca', 'es', 'ar', 'en', 'da', 'hr', 'sw', 'ro', 'af', 'cs', 'sk', 'vi', 'lt', 'sl', 'so', 'no', 'tl', 'pt', 'hu', 'id', 'et', 'it', 'sv', 'fi', 'cy'}\n"
     ]
    }
   ],
   "source": [
    "#HOW MANY LANGUAGES?\n",
    "print(f\"SHAPE -- > {df_twitter_text.shape}\")\n",
    "print(f\"HOW MANY LANGUAGES -- > {len(set(df_twitter_text['lang']))}\")\n",
    "print(set(df_twitter_text['lang']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i will not give context, i just find this imag...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apenas um treino usando uma gostosa</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foreign types with the hookah pipes say ay oh ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Embedded_text lang\n",
       "0  i will not give context, i just find this imag...   en\n",
       "1                Apenas um treino usando uma gostosa   pt\n",
       "2  MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...   en\n",
       "3  Foreign types with the hookah pipes say ay oh ...   en\n",
       "4  I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...   en"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FILTERING ENGLISH AND PORTUGUESE\n",
    "df_twitter_text = df_twitter_text.loc[df_twitter_text['lang'].isin(['en', 'pt'])]\n",
    "df_twitter_text.shape\n",
    "df_twitter_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEPARATING BETWEEN en AND pt TO TRANSLATE en\n",
    "df_twitter_text_en = df_twitter_text[df_twitter_text['lang']=='en'].set_index('lang')\n",
    "df_twitter_text_pt = df_twitter_text[df_twitter_text['lang']=='pt'].set_index('lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedded_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>i will not give context, i just find this imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>Foreign types with the hookah pipes say ay oh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>smelling my brother's bottle of whey protein p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>who up soundin their fury rn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>Working hard on my crossfit.\\n#CatsOfTwitter #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>Change your life. \\n\\nAnyone can do it. Start ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>Shit is about to get incredibly annoying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>Calories in Egg whites Vs Whey Protein.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Embedded_text\n",
       "lang                                                   \n",
       "en    i will not give context, i just find this imag...\n",
       "en    MONTY'S APE WATCH  Mega \"UNDER 10 $SOL\"  Editi...\n",
       "en    Foreign types with the hookah pipes say ay oh ...\n",
       "en    I use to have my autocorrect change ‚Äúway‚Äù to ‚Äú...\n",
       "en    smelling my brother's bottle of whey protein p...\n",
       "...                                                 ...\n",
       "en                         who up soundin their fury rn\n",
       "en    Working hard on my crossfit.\\n#CatsOfTwitter #...\n",
       "en    Change your life. \\n\\nAnyone can do it. Start ...\n",
       "en             Shit is about to get incredibly annoying\n",
       "en              Calories in Egg whites Vs Whey Protein.\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedded_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>Apenas um treino usando uma gostosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>Tinha mais gente no treino que no jogo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>saudade do crossfit dms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>Hoje eu n√£o acabei o treino foi ele quem acabo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>| Que fofura! \\n\\nJaqueline e Murilo levaram ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>escuta kpop no treino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>#eleuthero\\n#suplementos\\n#sistemarespirat√≥rio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>VEJA: Rodolffo (BBB 21) chama aten√ß√£o por fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>treino assim pra quem n√£o sabe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>O TREINO INOVADOR DO CORINTHIANS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2654 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Embedded_text\n",
       "lang                                                   \n",
       "pt                  Apenas um treino usando uma gostosa\n",
       "pt              Tinha mais gente no treino que no jogo.\n",
       "pt                              saudade do crossfit dms\n",
       "pt    Hoje eu n√£o acabei o treino foi ele quem acabo...\n",
       "pt     | Que fofura! \\n\\nJaqueline e Murilo levaram ...\n",
       "...                                                 ...\n",
       "pt                                escuta kpop no treino\n",
       "pt    #eleuthero\\n#suplementos\\n#sistemarespirat√≥rio...\n",
       "pt     VEJA: Rodolffo (BBB 21) chama aten√ß√£o por fal...\n",
       "pt                       treino assim pra quem n√£o sabe\n",
       "pt                     O TREINO INOVADOR DO CORINTHIANS\n",
       "\n",
       "[2654 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_twitter_text_en)\n",
    "display(df_twitter_text_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING TO CSV --> TXT --> WORD COUNT PYSPARK \n",
    "df_twitter_text_pt.to_csv('C:\\\\Users\\\\cedua\\\\CDIA - PUCSP\\\\PROJETO PySpark - SAVINO\\\\Contador-de-palavras---PySpark\\\\collecting_data/twitter_pt_data.csv', index=False)\n",
    "df_twitter_text_en.to_csv('C:\\\\Users\\\\cedua\\\\CDIA - PUCSP\\\\PROJETO PySpark - SAVINO\\\\Contador-de-palavras---PySpark\\\\collecting_data/twitter_en_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_twitter_data_PT(txt_path):\n",
    "    #pyspark session\n",
    "    spark = SparkSession.builder.appName(\"PySpark-LimpandoTwitterDataPT\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    #txt doc\n",
    "    df = spark.read.text(f\"{txt_path}\")\n",
    "\n",
    "    #spliting the df by space \" \" \n",
    "    df = df.select(\n",
    "    split(df.value, ' ').alias('words'))\n",
    "\n",
    "    #extracting words from the lists\n",
    "    df = df.select(\n",
    "    explode(col(\"words\")).alias(\"words3\"))\n",
    "    \n",
    "    #every word to lower case\n",
    "    df = df.select(\n",
    "    lower(\n",
    "    col('words3')\n",
    "    ).alias('words4'))\n",
    "\n",
    "    #removing punctuation\n",
    "    punc = r\"\"\"[|-:!?.,\"'\"\\\"\\/]\"\"\"\n",
    "    √©_rem = r\"\\b(\\w*√©\\w*)\"\n",
    "    https_rem = r\"(http|https)://[^\\s]*\"\n",
    "\n",
    "    df = df.withColumn(\"words5\", regexp_replace(\"words4\", punc, \"\"))\n",
    "    df = df.withColumn(\"words77\", regexp_replace(\"words5\", https_rem, \"\"))\n",
    "    df = df.withColumn(\"words7\", regexp_replace(\"words77\", √©_rem, \"\"))\n",
    "    \n",
    "    # Removing the NULLs\n",
    "    df = df.where(\n",
    "        col(\"words7\") != \"\")\n",
    "\n",
    "    #FIRST COUNT#\n",
    "    df_counted = df.groupBy(\n",
    "        col(\"words7\"),\n",
    "    ).count()\n",
    "\n",
    "    #RETAKING\n",
    "    df2 = df.select(\n",
    "        split(\n",
    "            col(\"words7\"),\n",
    "            \" \"\n",
    "        ).alias(\"word8\"))\n",
    "    \n",
    "    #removing stopwords - portuguese\n",
    "    stopwords_remover = StopWordsRemover(inputCol=\"word8\", outputCol=\"words9\",   \n",
    "                                            stopWords=StopWordsRemover.loadDefaultStopWords(\"portuguese\"))\n",
    "    df2 = stopwords_remover.transform(df2)\n",
    "    df2 = df2.drop(\"word8\")\n",
    "\n",
    "    #removing nulls - step2\n",
    "    df2 = df2.select(\n",
    "        (col(\"words9\")[0]).alias(\"twitter_text_data_PT\")) #LAST EDIT COL\n",
    "\n",
    "    df2 = df2.where(\n",
    "    col(\"twitter_text_data_PT\") != \"null\")\n",
    "\n",
    "    df2 = df2.where(\n",
    "        col(\"twitter_text_data_PT\") != \"\")\n",
    "    \n",
    "    #FINAL COUNT#\n",
    "    df_counted2 = df2.groupby(\n",
    "    col(\"twitter_text_data_PT\")\n",
    "    ).count()\n",
    "\n",
    "    df_counted2 = df_counted2.orderBy(desc(\"count\")).toPandas()\n",
    "    \n",
    "    return df_counted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_twitter_data_EN(txt_path):\n",
    "    #pyspark session\n",
    "    spark = SparkSession.builder.appName(\"PySpark-LimpandoTwitterDataEN\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    #txt doc\n",
    "    df = spark.read.text(f\"{txt_path}\")\n",
    "\n",
    "    #spliting the df by space \" \" \n",
    "    df = df.select(\n",
    "    split(df['value'], ' ').alias('words'))\n",
    "    \n",
    "    #extracting words from the lists\n",
    "    df = df.select(\n",
    "    explode(col(\"words\")).alias(\"words3\"))\n",
    "    \n",
    "    #every word to lower case\n",
    "    df = df.select(\n",
    "    lower(\n",
    "    col('words3')\n",
    "    ).alias('words44'))\n",
    "\n",
    "    #removing punctuation\n",
    "    https_rem = r\"(http|https)://[^\\s]*\"\n",
    "    df = df.withColumn(\"words4\", regexp_replace(\"words44\", https_rem, \"\"))\n",
    "\n",
    "    df = df.select(\n",
    "    regexp_extract(\n",
    "    col('words4'),  #cleaning words2 column\n",
    "    '[A-z]+',       #selecting all the words\n",
    "    0).alias('words5'))\n",
    "\n",
    "    # Removing the NULLs\n",
    "    df = df.where(\n",
    "        col(\"words5\") != \"\")\n",
    "    \n",
    "    #FIRST COUNT#\n",
    "    df_counted = df.groupBy(\n",
    "        col(\"words5\"),\n",
    "    ).count()\n",
    "\n",
    "    #RETAKING\n",
    "    df2 = df.select(\n",
    "        split(\n",
    "            col(\"words5\"),\n",
    "            \" \"\n",
    "        ).alias(\"word6\"))\n",
    "    \n",
    "    #removing stopwords - english \n",
    "    stopwords_remover2 = StopWordsRemover(inputCol=\"word6\", outputCol=\"words7\")\n",
    "    df2 = stopwords_remover2.transform(df2)\n",
    "    df2 = df2.drop(\"word6\")\n",
    "\n",
    "    #removing nulls - step2\n",
    "    df2 = df2.select(\n",
    "        (col(\"words7\")[0]).alias(\"twitter_text_data_EN\"))\n",
    "\n",
    "    df2 = df2.where(\n",
    "    col(\"twitter_text_data_EN\") != \"null\")\n",
    "\n",
    "    df2 = df2.where(\n",
    "        col(\"twitter_text_data_EN\") != \"\")\n",
    "    \n",
    "    #FINAL COUNT#\n",
    "    df_counted2 = df2.groupby(\n",
    "    col(\"twitter_text_data_EN\")\n",
    "    ).count()\n",
    "\n",
    "    df_counted2 = df_counted2.orderBy(desc(\"count\")).toPandas()\n",
    "    \n",
    "    return df_counted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o127.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (DESKTOP-JO1VKGV executor driver): org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.PATTERN] The value of parameter(s) `regexp` in `regexp_replace` is invalid: '[|-:!?.,\"\\'\"\\\\\"\\\\/]'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidPatternError(QueryExecutionErrors.scala:2754)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidPatternError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.util.regex.PatternSyntaxException: Illegal character range near index 3\r\n[|-:!?.,\"'\"\\\"\\/]\r\n   ^\r\n\tat java.util.regex.Pattern.error(Unknown Source)\r\n\tat java.util.regex.Pattern.range(Unknown Source)\r\n\tat java.util.regex.Pattern.clazz(Unknown Source)\r\n\tat java.util.regex.Pattern.sequence(Unknown Source)\r\n\tat java.util.regex.Pattern.expr(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.util.regex.Pattern.<init>(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\t... 18 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.PATTERN] The value of parameter(s) `regexp` in `regexp_replace` is invalid: '[|-:!?.,\"\\'\"\\\\\"\\\\/]'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidPatternError(QueryExecutionErrors.scala:2754)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidPatternError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.util.regex.PatternSyntaxException: Illegal character range near index 3\r\n[|-:!?.,\"'\"\\\"\\/]\r\n   ^\r\n\tat java.util.regex.Pattern.error(Unknown Source)\r\n\tat java.util.regex.Pattern.range(Unknown Source)\r\n\tat java.util.regex.Pattern.clazz(Unknown Source)\r\n\tat java.util.regex.Pattern.sequence(Unknown Source)\r\n\tat java.util.regex.Pattern.expr(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.util.regex.Pattern.<init>(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\t... 18 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_twitter_text_pt_pyspark \u001b[39m=\u001b[39m clean_twitter_data_PT(\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcedua\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mCDIA - PUCSP\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mPROJETO PySpark - SAVINO\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mContador-de-palavras---PySpark\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mcollecting_data\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mtwitter_pt_data.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m df_twitter_text_pt_pyspark\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mcedua\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCDIA - PUCSP\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mPROJETO PySpark - SAVINO\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mContador-de-palavras---PySpark\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mcollecting_data\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtwitter_pt_data_pyspark.csv\u001b[39m\u001b[39m'\u001b[39m,index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m df_twitter_text_en_pyspark \u001b[39m=\u001b[39m clean_twitter_data_EN(\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mcedua\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCDIA - PUCSP\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mPROJETO PySpark - SAVINO\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mContador-de-palavras---PySpark\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mcollecting_data\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtwitter_en_data.txt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36mclean_twitter_data_PT\u001b[1;34m(txt_path)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m#FINAL COUNT#\u001b[39;00m\n\u001b[0;32m     65\u001b[0m df_counted2 \u001b[39m=\u001b[39m df2\u001b[39m.\u001b[39mgroupby(\n\u001b[0;32m     66\u001b[0m col(\u001b[39m\"\u001b[39m\u001b[39mtwitter_text_data_PT\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m )\u001b[39m.\u001b[39mcount()\n\u001b[1;32m---> 69\u001b[0m df_counted2 \u001b[39m=\u001b[39m df_counted2\u001b[39m.\u001b[39;49morderBy(desc(\u001b[39m\"\u001b[39;49m\u001b[39mcount\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39;49mtoPandas()\n\u001b[0;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m df_counted2\n",
      "File \u001b[1;32mc:\\Users\\cedua\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m pdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect(), columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m    209\u001b[0m column_counter \u001b[39m=\u001b[39m Counter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\cedua\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m \n\u001b[0;32m   1198\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[1;32m-> 1216\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[0;32m   1217\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\cedua\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\cedua\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\cedua\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o127.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (DESKTOP-JO1VKGV executor driver): org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.PATTERN] The value of parameter(s) `regexp` in `regexp_replace` is invalid: '[|-:!?.,\"\\'\"\\\\\"\\\\/]'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidPatternError(QueryExecutionErrors.scala:2754)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidPatternError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.util.regex.PatternSyntaxException: Illegal character range near index 3\r\n[|-:!?.,\"'\"\\\"\\/]\r\n   ^\r\n\tat java.util.regex.Pattern.error(Unknown Source)\r\n\tat java.util.regex.Pattern.range(Unknown Source)\r\n\tat java.util.regex.Pattern.clazz(Unknown Source)\r\n\tat java.util.regex.Pattern.sequence(Unknown Source)\r\n\tat java.util.regex.Pattern.expr(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.util.regex.Pattern.<init>(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\t... 18 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.PATTERN] The value of parameter(s) `regexp` in `regexp_replace` is invalid: '[|-:!?.,\"\\'\"\\\\\"\\\\/]'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidPatternError(QueryExecutionErrors.scala:2754)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidPatternError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.util.regex.PatternSyntaxException: Illegal character range near index 3\r\n[|-:!?.,\"'\"\\\"\\/]\r\n   ^\r\n\tat java.util.regex.Pattern.error(Unknown Source)\r\n\tat java.util.regex.Pattern.range(Unknown Source)\r\n\tat java.util.regex.Pattern.clazz(Unknown Source)\r\n\tat java.util.regex.Pattern.sequence(Unknown Source)\r\n\tat java.util.regex.Pattern.expr(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.util.regex.Pattern.<init>(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\t... 18 more\r\n"
     ]
    }
   ],
   "source": [
    "df_twitter_text_pt_pyspark = clean_twitter_data_PT(\"C:\\\\Users\\cedua\\\\CDIA - PUCSP\\\\PROJETO PySpark - SAVINO\\\\Contador-de-palavras---PySpark\\\\collecting_data\\\\twitter_pt_data.txt\")\n",
    "df_twitter_text_pt_pyspark.to_csv('C:\\\\Users\\\\cedua\\\\CDIA - PUCSP\\\\PROJETO PySpark - SAVINO\\\\Contador-de-palavras---PySpark\\\\collecting_data\\\\twitter_pt_data_pyspark.csv',index=False)\n",
    "\n",
    "df_twitter_text_en_pyspark = clean_twitter_data_EN(\"C:\\\\Users\\cedua\\\\CDIA - PUCSP\\\\PROJETO PySpark - SAVINO\\\\Contador-de-palavras---PySpark\\\\collecting_data\\\\twitter_en_data.txt\")\n",
    "df_twitter_text_en_pyspark.to_csv('C:\\\\Users\\\\cedua\\\\CDIA - PUCSP\\\\PROJETO PySpark - SAVINO\\\\Contador-de-palavras---PySpark\\\\collecting_data\\\\twitter_en_data_pyspark.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddff = pd.read_csv('C:\\\\Users\\\\cedua\\\\CDIA - PUCSP\\\\PROJETO PySpark - SAVINO\\\\Contador-de-palavras---PySpark\\\\collecting_data/twitter_pt_data_pyspark.csv')\n",
    "# ddff.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter data PT\n",
    "- remover links\n",
    "- remover | , \\ , - , etc\n",
    "\n",
    "twitter data EN \n",
    "- remover links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
