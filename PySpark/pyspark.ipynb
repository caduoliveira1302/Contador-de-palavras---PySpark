{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cedua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode, col, trim, rtrim, ltrim\n",
    "from pyspark.sql.functions import regexp_replace, regexp_extract\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_EN(txt_path):\n",
    "    #pyspark session\n",
    "    spark = SparkSession.builder.appName(\"PySpark-ContandoPalavras\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    #txt doc\n",
    "    df = spark.read.text(f\"{txt_path}\")\n",
    "\n",
    "    #spliting the df by space \" \" \n",
    "    df = df.select(\n",
    "    split(df['value'], ' ').alias('words'))\n",
    "    \n",
    "    #extracting words from the lists\n",
    "    df = df.select(\n",
    "    explode(col(\"words\")).alias(\"words3\"))\n",
    "    \n",
    "    #every word to lower case\n",
    "    df = df.select(\n",
    "    lower(\n",
    "    col('words3')\n",
    "    ).alias('words4'))\n",
    "\n",
    "    #removing punctuation\n",
    "    df = df.select(\n",
    "    regexp_extract(\n",
    "    col('words4'),  #cleaning words2 column\n",
    "    '[A-z]+',       #selecting all the words\n",
    "    0).alias('words5'))\n",
    "\n",
    "    # Removing the NULLs\n",
    "    df = df.where(\n",
    "        col(\"words5\") != \"\")\n",
    "    \n",
    "    #FIRST COUNT#\n",
    "    df_counted = df.groupBy(\n",
    "        col(\"words5\"),\n",
    "    ).count()\n",
    "\n",
    "    #RETAKING\n",
    "    df2 = df.select(\n",
    "        split(\n",
    "            col(\"words5\"),\n",
    "            \" \"\n",
    "        ).alias(\"word6\"))\n",
    "    \n",
    "    #removing stopwords - english \n",
    "    stopwords_remover2 = StopWordsRemover(inputCol=\"word6\", outputCol=\"words7\")\n",
    "    df2 = stopwords_remover2.transform(df2)\n",
    "    df2 = df2.drop(\"word6\")\n",
    "\n",
    "    #removing nulls - step2\n",
    "    df2 = df2.select(\n",
    "        (col(\"words7\")[0]).alias(\"words8\"))\n",
    "\n",
    "    df2 = df2.where(\n",
    "    col(\"words8\") != \"null\")\n",
    "\n",
    "    df2 = df2.where(\n",
    "        col(\"words8\") != \"\")\n",
    "    \n",
    "    #FINAL COUNT#\n",
    "    df_counted2 = df2.groupby(\n",
    "    col(\"words8\")\n",
    "    ).count()\n",
    "\n",
    "    df_counted2 = df_counted2.orderBy(desc(\"count\")).toPandas()\n",
    "    \n",
    "    return df_counted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_PT(txt_path):\n",
    "    #pyspark session\n",
    "    spark = SparkSession.builder.appName(\"PySpark-ContandoPalavras\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    #txt doc\n",
    "    df = spark.read.text(f\"{txt_path}\")\n",
    "\n",
    "    #spliting the df by space \" \" \n",
    "    df = df.select(\n",
    "    split(df.value, ' ').alias('words'))\n",
    "\n",
    "    #extracting words from the lists\n",
    "    df = df.select(\n",
    "    explode(col(\"words\")).alias(\"words3\"))\n",
    "    \n",
    "    #every word to lower case\n",
    "    df = df.select(\n",
    "    lower(\n",
    "    col('words3')\n",
    "    ).alias('words4'))\n",
    "\n",
    "    #removing punctuation\n",
    "    punc = r\"\"\"[:!?.,\"'\"\\\"\\/]\"\"\"\n",
    "    é_rem = r\"\\b(\\w*é\\w*)\"\n",
    "    pode_rem = r\"pode\"\n",
    "    ser_rem = r\"ser\"\n",
    "    df = df.withColumn(\"words5\", regexp_replace(\"words4\", punc, \"\"))\n",
    "    df = df.withColumn(\"words6\", regexp_replace(\"words5\", é_rem, \"\"))\n",
    "    df = df.withColumn(\"words1\", regexp_replace(\"words6\", pode_rem, \"\"))\n",
    "    df = df.withColumn(\"words7\", regexp_replace(\"words1\", ser_rem, \"\"))\n",
    "    \n",
    "    # Removing the NULLs\n",
    "    df = df.where(\n",
    "        col(\"words7\") != \"\")\n",
    "\n",
    "    #FIRST COUNT#\n",
    "    df_counted = df.groupBy(\n",
    "        col(\"words7\"),\n",
    "    ).count()\n",
    "\n",
    "    #RETAKING\n",
    "    df2 = df.select(\n",
    "        split(\n",
    "            col(\"words7\"),\n",
    "            \" \"\n",
    "        ).alias(\"word8\"))\n",
    "    \n",
    "    #removing stopwords - portuguese\n",
    "    stopwords_remover = StopWordsRemover(inputCol=\"word8\", outputCol=\"words9\", \n",
    "                                            stopWords=StopWordsRemover.loadDefaultStopWords(\"portuguese\"))\n",
    "    df2 = stopwords_remover.transform(df2)\n",
    "    df2 = df2.drop(\"word8\")\n",
    "\n",
    "    #removing nulls - step2\n",
    "    df2 = df2.select(\n",
    "        (col(\"words9\")[0]).alias(\"words10\"))\n",
    "\n",
    "    df2 = df2.where(\n",
    "    col(\"words10\") != \"null\")\n",
    "\n",
    "    df2 = df2.where(\n",
    "        col(\"words10\") != \"\")\n",
    "    \n",
    "    #FINAL COUNT#\n",
    "    df_counted2 = df2.groupby(\n",
    "    col(\"words10\")\n",
    "    ).count()\n",
    "\n",
    "    df_counted2 = df_counted2.orderBy(desc(\"count\")).toPandas()\n",
    "    \n",
    "    return df_counted2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving all the word_count.txt files in a CSV's files for future analysis and dashboard creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **EN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder path\n",
    "folder_path = 'C:/Users/cedua/CDIA - PUCSP/PROJETO PySpark - SAVINO/Contador-de-palavras---PySpark/data/data_en'\n",
    "\n",
    "#iterating the txt files only\n",
    "for txt_file in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "    df_pandas = count_words_EN(txt_file)\n",
    "    df_pandas.to_csv(f'{txt_file[:-4]}.csv', sep=';', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **PT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder path\n",
    "folder_path = 'C:/Users/cedua/CDIA - PUCSP/PROJETO PySpark - SAVINO/Contador-de-palavras---PySpark/data/data_pt'\n",
    "\n",
    "#iterating the txt files only\n",
    "for txt_file in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "    df_pandas = count_words_PT(txt_file)\n",
    "    df_pandas.to_csv(f'{txt_file[:-4]}.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
